[
{
	"uri": "//localhost:1313/3-eks/3.1-eks-role/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/3-eks/3.3-eks-create/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/3-eks/3.4-eks-worker-node-create/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/3-eks/3.5-ecr/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/3-eks/3.6-run-terraform/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.2-jenkins/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.3-dev-repo/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.4-argocd-autodeploy/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.5-prometheus-grafana/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.6-efk/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/5-devsecops/5.2-sonar/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/5-devsecops/5.3-jenkins/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/5-devsecops/5.4-trivy-owasp-zap/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/2.2.1-key/",
	"title": "Create Load Balancer and Target Group",
	"tags": [],
	"description": "",
	"content": "Create Load Balancer First, we create Security Group for our Load Balancer. Create alb.tf with the configuration below:\n# Security Group for ALB resource \u0026#34;aws_security_group\u0026#34; \u0026#34;ALB_SG\u0026#34; { name = \u0026#34;ALB SG\u0026#34; description = \u0026#34;Allow HTTP inbound and all outbound traffic\u0026#34; vpc_id = aws_vpc.main.id ingress = [ { from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] description = \u0026#34;Allow inbound traffic on port 80\u0026#34; ipv6_cidr_blocks = [] prefix_list_ids = [] security_groups = [] self = false }, ] egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = { Name = \u0026#34;ALB SG\u0026#34; } } Now, we created Security Group for Load Balancer. This Security Group allows all traffic from port 80 of the Load Balancer.\nNext, we will create the Load Balancer. In file alb.tf, adding these configuration below:\nresource \u0026#34;aws_alb\u0026#34; \u0026#34;alb_vlsm\u0026#34; { name = \u0026#34;ALB-ECS\u0026#34; internal = false load_balancer_type = \u0026#34;application\u0026#34; security_groups = [aws_security_group.ALB_SG.id] subnets = [aws_subnet.public_subnet_1.id, aws_subnet.public_subnet_2.id] # enable_deletion_protection = false tags = { Environment = \u0026#34;production\u0026#34; } } Create Target Group Load Balancer will perform check health on all targets in the Target Group and traffic will be only fowarded to \u0026ldquo;healthy\u0026rdquo; target.\nIn case all targets are considered unhealthy, Load Balancer still forwards traffic to those targets. Please note this!\nIn alb.sg, adding the additional configuration as below:\n## Target Group for our service resource \u0026#34;aws_alb_target_group\u0026#34; \u0026#34;alb_target_group\u0026#34; { name = \u0026#34;TargetGroupForService\u0026#34; port = \u0026#34;80\u0026#34; protocol = \u0026#34;HTTP\u0026#34; vpc_id = aws_vpc.main.id deregistration_delay = 120 health_check { healthy_threshold = \u0026#34;2\u0026#34; unhealthy_threshold = \u0026#34;2\u0026#34; interval = \u0026#34;60\u0026#34; path = \u0026#34;/\u0026#34; port = \u0026#34;traffic-port\u0026#34; protocol = \u0026#34;HTTP\u0026#34; timeout = \u0026#34;30\u0026#34; } } The Load Balancer will send HTTP requests to all targets every 60 seconds. After 2 consecutive successful responses (status 200 by default, depending on how it\u0026rsquo;s configured), the target will be considered Healthy. In contrast, the target is marked as \u0026lsquo;Unhealthy\u0026rsquo; if it fails to respond successfully for 2 consecutive checks.\nConfigure Load Balancer Listener and Listener Rule We need to configure Listener for Load Balancer. Listener is the process of listening traffic on a configured port. For example, Load Balancer will listen on port 80 (for HTTP request) and port 443 (for HTTPS request)\nIn alb.tf, adding the configuration as below:\nresource \u0026#34;aws_alb_listener\u0026#34; \u0026#34;alb_listener_http\u0026#34; { load_balancer_arn = aws_alb.alb_vlsm.arn port = \u0026#34;80\u0026#34; protocol = \u0026#34;HTTP\u0026#34; default_action { type = \u0026#34;fixed-response\u0026#34; fixed_response { content_type = \u0026#34;text/plain\u0026#34; message_body = \u0026#34;Access denied\u0026#34; status_code = \u0026#34;403\u0026#34; } } } resource \u0026#34;aws_alb_listener_rule\u0026#34; \u0026#34;alb_listener_http\u0026#34; { listener_arn = aws_alb_listener.alb_listener_http.arn action { type = \u0026#34;forward\u0026#34; target_group_arn = aws_alb_target_group.alb_target_group.arn } condition { path_pattern { values = [\u0026#34;/*\u0026#34;] } } } "
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.1-vpc/",
	"title": "Create VPC and Bastion Host",
	"tags": [],
	"description": "",
	"content": "In this section, we will create a VPC with 2 private/public subnets, an Internet Gateway, and a NAT Gateway. Then, we launch a Bastion Host into public subnet 2.\nNội dung Create VPC, Internet Gateway, and NAT Gateway Create Subnet Subnet Association Create Public and Private Key Create Bastion Host Execute Terraform "
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.1-vpc/2.1.1-createvpc/",
	"title": "Create VPC, Internet Gateway, and NAT Gateway",
	"tags": [],
	"description": "",
	"content": "Creating vpc.tf with the configuration below:\n# vpc.tf resource \u0026#34;aws_vpc\u0026#34; \u0026#34;main\u0026#34; { cidr_block = var.vpc_cidr enable_dns_hostnames = true enable_dns_support = true tags = { Name = var.vpc_name } } resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;defaultIGW\u0026#34; { vpc_id = aws_vpc.main.id tags = { Name = \u0026#34;Workshop 1 Internet Gateway\u0026#34; } } # Elastic IP for NAT Gateway resource \u0026#34;aws_eip\u0026#34; \u0026#34;my_elastic_ip\u0026#34; { domain = \u0026#34;vpc\u0026#34; } # NAT Gateway resource \u0026#34;aws_nat_gateway\u0026#34; \u0026#34;my_nat_gtw\u0026#34; { allocation_id = aws_eip.my_elastic_ip.id subnet_id = aws_subnet.public_subnet_1.id tags = { Name = \u0026#34;NAT Gateway\u0026#34; } } First, we create a VPC with a defined CIDR Block (Please follow this link to check any defined variables). Then, we create an Internet Gateway and attach it to our VPC.\nFinally, an Elastic IP is created and attached to the NAT Gateway so that private instances can reach the internet.\n"
},
{
	"uri": "//localhost:1313/",
	"title": "Implementing DevSecOps Practice and GitOps Principal in CI/CD Pipeline for a Microservice Application running on AWS EKS",
	"tags": [],
	"description": "",
	"content": "Implementing DevSecOps Practice and GitOps Principal in CI/CD Pipeline for a Microservice Application running on AWS EKS Overview In this workshop, we will provide step-by-step instructions for deploying a ReactJS Application into AWS Infrastructure, specifically Elastic Container Service (ECS). However, we will not interact with the Console Management since we have to manage too many resources ourselves. Infrastructure as Code (IaC) provides us the ability to provision the infrastructure without manual operation. IaC improves productivity in the process of problem-addressing and troubleshooting since the configuration is well-defined. In this workshop, we will use Terraform to deploy our AWS Infrastructure.\nExpectation After deploying the ECS Cluster, end-users will use the DNS of the Load Balancer to access the application. Our application is a VLSM Solver, which is a classic problem in computer networking. You can follow this link to experience the deployed application. For your preference, below are the repositories for the Back End and Front End.\nBackEnd Repo: Link FrontEnd Repo: Link In case you want the entire source code, please follow this link.\nContent Problem Definition Preparation Scaling Testing Cleanup "
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Problem Definition",
	"tags": [],
	"description": "",
	"content": "Containerizing an application and deploying it into a server seems to solve the problem of dependencies since there is more than one application running on a single server. However, there are some issues:\nHow does the server handle hundreds or millions of incoming requests? Suppose there are 2 Services (A and B) operating on the same server. End-users tend to access Service A more than they do on Service B. So we need to provide computing resources for Service A to avoid any heavy workload. In this workshop, we will use these services from AWS to solve the problem above:\nAuto Scaling: Automatically Scale based on the CPU performance Load Balancer: Distribute incoming traffic to servers Elastic Container Service: Utilizing the container management "
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/",
	"title": "Create Load Balancer, Auto Scaling Group, and ECS Cluster",
	"tags": [],
	"description": "",
	"content": "In this section, we will create the entire AWS Infratructure with Load Balancer, Auto Scaling Group, and ECS Cluster.\nContent Create Load Balancer and Target Group Create Template for EC2 belonging to ECS Cluster Create Auto Scaling Group Create ECS Cluster "
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.1-vpc/2.1.2-create-subnet/",
	"title": "Create Subnet",
	"tags": [],
	"description": "",
	"content": "Create Public and Private subnet Create subnet.tf with the configuration below:\n# subnet.tf resource \u0026#34;aws_subnet\u0026#34; \u0026#34;public_subnet_1\u0026#34; { vpc_id = aws_vpc.main.id cidr_block = var.vpc_public_subnets[0] availability_zone = var.vpc_azs[0] tags = { Name = \u0026#34;Public Subnet 1\u0026#34; } } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;public_subnet_2\u0026#34; { vpc_id = aws_vpc.main.id cidr_block = var.vpc_public_subnets[1] availability_zone = var.vpc_azs[1] tags = { Name = \u0026#34;Public Subnet 2\u0026#34; } } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private_subnet_1\u0026#34; { vpc_id = aws_vpc.main.id cidr_block = var.vpc_private_subnets[0] availability_zone = var.vpc_azs[0] tags = { Name = \u0026#34;Private Subnet 1\u0026#34; } } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private_subnet_2\u0026#34; { vpc_id = aws_vpc.main.id cidr_block = var.vpc_private_subnets[1] availability_zone = var.vpc_azs[1] tags = { Name = \u0026#34;Private Subnet 2\u0026#34; } } Finally, we have 4 subnets in total as below:\nPublic Subnet 1 (CIDR Block: 10.0.1.0/24) Private Subnet 1 (CIDR Block: 10.0.3.0/24) Public Subnet 2 (CIDR Block: 10.0.2.0/24) Private Subnet 2 (CIDR Block: 10.0.4.0/24) Please follow this link to check any defined variables\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/2.2.2-jump-host/",
	"title": "Create Template for EC2 belonging to ECS Cluster",
	"tags": [],
	"description": "",
	"content": "Create Security Group We need to create template for all EC2 belonging to the ECS Cluster. All container will run on these EC2 Instance. From now, let\u0026rsquo;s call them EC2 Cluster. But first, we will create Security Group for EC2 Cluster Create ecs_ec2.tf with the configuration below:\n# ecs_ec2.tf resource \u0026#34;aws_security_group\u0026#34; \u0026#34;ECS_EC2_SG\u0026#34;{ name = \u0026#34;ECS_EC2_SG\u0026#34; description = \u0026#34;Allow traffic from ALB_SG and 22 from Bastion\u0026#34; vpc_id = aws_vpc.main.id ingress = [ { from_port = 49153 to_port = 65535 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [] description = \u0026#34;Allow all traffic coming from port 49153-65535 from ALB\u0026#34; ipv6_cidr_blocks = [] prefix_list_ids = [] security_groups = [aws_security_group.ALB_SG.id] self = false }, { from_port = 32768 to_port = 61000 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [] description = \u0026#34;Allow all traffic coming from port 32768-61000 from ALB\u0026#34; ipv6_cidr_blocks = [] prefix_list_ids = [] security_groups = [aws_security_group.ALB_SG.id] self = false }, { from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [] description = \u0026#34;Allow SSH traffic from Bastion host\u0026#34; ipv6_cidr_blocks = [] prefix_list_ids = [] security_groups = [aws_security_group.Bastion_SG.id] self = false } ] egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = { Name = \u0026#34;ECS EC2 SG\u0026#34; } } With the above Security Group, we have this configuration:\nEC2 Cluster accepts all inbound traffic (from Load Balancer) on range ports 49153-65535 EC2 Cluster accepts all inbound traffic (from Load Balancer) on range ports 32768-61000 EC2 Cluster accepts SSH Protocol from Bastion Host EC2 Cluster allows all outbound traffic You may wonder why we have ranges port 49153-65535 and 32768-61000. The answer is that the container will listen to these range ports after it is successfully launched on the EC2 Cluster. There is a terminology for these ports: ephemeral port.\nFor example, given an ECS Cluster containing 2 EC2 (1) and (2), then:\nOn EC2 (1): there are 3 running containers, which listen on ports 49153, 49154, and 49155, respectively. On EC2 (2): there are 3 running containers, which listen on ports 32769, 32770, and 32771, respectively. Create IAM Role for EC2 Cluster We need to create IAM Role for EC2 Cluster to allow them communicating with the ECS Cluster. Create iamrole.tf with the configuration as below:\ndata \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;ec2_instance_role_policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRole\u0026#34;] effect = \u0026#34;Allow\u0026#34; principals { type = \u0026#34;Service\u0026#34; identifiers = [ \u0026#34;ec2.amazonaws.com\u0026#34;, \u0026#34;ecs.amazonaws.com\u0026#34; ] } } } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;ec2_instance_role_policy\u0026#34; { role = aws_iam_role.ec2_instance_role.name policy_arn = \u0026#34;arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\u0026#34; } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;ec2_instance_role\u0026#34; { name = \u0026#34;ECS_EC2_InstanceRole\u0026#34; assume_role_policy = data.aws_iam_policy_document.ec2_instance_role_policy.json } resource \u0026#34;aws_iam_instance_profile\u0026#34; \u0026#34;ec2_instance_role_profile\u0026#34; { name = \u0026#34;EC2_InstanceRoleProfile\u0026#34; role = aws_iam_role.ec2_instance_role.id } Create AMI for EC2 Cluster Next, we will create user_data.sh as below. Our EC2 Cluster need to execute the command line below during the process of initialization. We need to configure file /etc/ecs/ecs.config to register these instance into the ECS Cluster.\n#!/bin/bash echo ECS_CLUSTER=\u0026#39;${ecs_cluster_name}\u0026#39; \u0026gt;\u0026gt; /etc/ecs/ecs.config Create ecs_ec2.tf as below:\ndata \u0026#34;aws_ami\u0026#34; \u0026#34;amazon_linux_2\u0026#34; { most_recent = true filter { name = \u0026#34;virtualization-type\u0026#34; values = [\u0026#34;hvm\u0026#34;] } filter { name = \u0026#34;owner-alias\u0026#34; values = [\u0026#34;amazon\u0026#34;] } filter { name = \u0026#34;name\u0026#34; values = [\u0026#34;amzn2-ami-ecs-hvm-*-x86_64-ebs\u0026#34;] } owners = [\u0026#34;amazon\u0026#34;] } ## Launch template for all EC2 instances that are part of the ECS cluster resource \u0026#34;aws_launch_template\u0026#34; \u0026#34;ecs_launch_template\u0026#34; { name = \u0026#34;ECS_EC2_LaunchTemplate\u0026#34; image_id = data.aws_ami.amazon_linux_2.id instance_type = var.ec2_instance_type key_name = aws_key_pair.EC2key.key_name user_data = base64encode(templatefile(\u0026#34;${path.module}/user_data.sh\u0026#34;, {ecs_cluster_name = aws_ecs_cluster.main.name}) ) vpc_security_group_ids = [aws_security_group.ECS_EC2_SG.id] iam_instance_profile { arn = aws_iam_instance_profile.ec2_instance_role_profile.arn } monitoring { enabled = true } } Now, aws_ecs_cluster.main.name is the name of our ECS Cluster, which will be specified in the next section.\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/",
	"title": "Preparation",
	"tags": [],
	"description": "",
	"content": "\rYou need to install Terraform before going further in this workshop\nBefore creating an ECS Cluster, we create some required components such as VPC, Subnets, Internet Gateway, etc. Then, we will create an Auto Scaling Group for ECS EC2 (containers will run on these instances), a Load Balancer to distribute incoming traffic and an ECS Cluster\nContent Create VPC and Bastion Host Create Autoscaling Group, Load Balancer, and ECS Cluster "
},
{
	"uri": "//localhost:1313/3-eks/3.2-eks-worker-role/",
	"title": "Validate Scaling Ability",
	"tags": [],
	"description": "",
	"content": "Content: Send 100.000 requests to Load Balancer Send 1.000.000 requests to Load Balancer Scale up Desired Task "
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/2.2.3-jenkins/",
	"title": "Create Auto Scaling Group",
	"tags": [],
	"description": "",
	"content": "Now, we will create Auto Scaling Group for EC2 Cluster. Create asg.tf with the configuration below:\nresource \u0026#34;aws_autoscaling_group\u0026#34; \u0026#34;ecs_autoscaling_group\u0026#34; { name = \u0026#34;ECS_ASG\u0026#34; max_size = 4 min_size = 2 desired_capacity = 2 vpc_zone_identifier = [aws_subnet.private_subnet_1.id, aws_subnet.private_subnet_2.id] health_check_type = \u0026#34;EC2\u0026#34; protect_from_scale_in = true enabled_metrics = [ \u0026#34;GroupMinSize\u0026#34;, \u0026#34;GroupMaxSize\u0026#34;, \u0026#34;GroupDesiredCapacity\u0026#34;, \u0026#34;GroupInServiceInstances\u0026#34;, \u0026#34;GroupPendingInstances\u0026#34;, \u0026#34;GroupStandbyInstances\u0026#34;, \u0026#34;GroupTerminatingInstances\u0026#34;, \u0026#34;GroupTotalInstances\u0026#34; ] launch_template { id = aws_launch_template.ecs_launch_template.id version = \u0026#34;$Latest\u0026#34; } instance_refresh { strategy = \u0026#34;Rolling\u0026#34; } tag { key = \u0026#34;Name\u0026#34; value = \u0026#34;ASG For ECS\u0026#34; propagate_at_launch = true } } With the configuration above, Auto Scaling Group (ASG) will launch EC2 Cluster into private subnet 1 and 2 based on the AMI, which is defined in the previous section. ASG guarantees that at least there are 2 EC2 running at the same time. In case of scaling up, there will be 4 EC2 Instances in the ECS Cluster. However, we need to configure that. In asg.tf, adding the configuration as below:\nresource \u0026#34;aws_autoscaling_policy\u0026#34; \u0026#34;asg_policy_ecs_autoscaling_group\u0026#34; { autoscaling_group_name = aws_autoscaling_group.ecs_autoscaling_group.name name = \u0026#34;Custom Policy\u0026#34; policy_type = \u0026#34;TargetTrackingScaling\u0026#34; target_tracking_configuration { predefined_metric_specification { predefined_metric_type = \u0026#34;ASGAverageCPUUtilization\u0026#34; } target_value = 50.0 } } In 3 minutes, if CPU Utilization reachs 50%, ASG will launch another EC2 Instance to reduce the heavy workload.\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.1-vpc/2.1.3-run-terraform/",
	"title": "Subnet Association",
	"tags": [],
	"description": "",
	"content": "Subnet Association Since we create VPC and subnet manually so we need to create route table and associate the subnets manually ourselves. If you want Terraform do it for you, you can use Module. However, this Workshop will help you create VPC from scratch in case you have no background in Terraform.\nCreate route_table.tf with the configuration below:\n# route_table.tf resource \u0026#34;aws_route_table\u0026#34; \u0026#34;public_route_table\u0026#34; { vpc_id = aws_vpc.main.id route { cidr_block = var.vpc_cidr gateway_id = \u0026#34;local\u0026#34; } route { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; gateway_id = aws_internet_gateway.defaultIGW.id } tags = { Name = \u0026#34;Public Route Table\u0026#34; } } resource \u0026#34;aws_route_table\u0026#34; \u0026#34;private_route_table\u0026#34; { vpc_id = aws_vpc.main.id tags = { Name = \u0026#34;Private Route Table\u0026#34; } } resource \u0026#34;aws_route\u0026#34; \u0026#34;route_nat_gw\u0026#34; { route_table_id = aws_route_table.private_route_table.id destination_cidr_block = \u0026#34;0.0.0.0/0\u0026#34; nat_gateway_id = aws_nat_gateway.my_nat_gtw.id timeouts { create = \u0026#34;5m\u0026#34; } } ## Subnet Association resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;public_subnet_1_associate\u0026#34; { subnet_id = aws_subnet.public_subnet_1.id route_table_id = aws_route_table.public_route_table.id } ## Subnet Association resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;public_subnet_2_associate\u0026#34; { subnet_id = aws_subnet.public_subnet_2.id route_table_id = aws_route_table.public_route_table.id } ## Subnet Association resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;private_subnet_1_associate\u0026#34; { subnet_id = aws_subnet.private_subnet_1.id route_table_id = aws_route_table.private_route_table.id } ## Subnet Association resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;private_subnet_2_associate\u0026#34; { subnet_id = aws_subnet.private_subnet_2.id route_table_id = aws_route_table.private_route_table.id } "
},
{
	"uri": "//localhost:1313/3-eks/",
	"title": "Validate Scaling",
	"tags": [],
	"description": "",
	"content": "In this step, we will validate the scalability of the infrastructure.\nContent Connect to Bastion Host Validate Scaling Ability "
},
{
	"uri": "//localhost:1313/4-cicd/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Using Terminal and executing this command lines:\nterraform destroy Now, all resources will be removed as below:\n"
},
{
	"uri": "//localhost:1313/5-devsecops/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Using Terminal and executing this command lines:\nterraform destroy Now, all resources will be removed as below:\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.1-application/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/5-devsecops/5.1-ops-repo/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/2.2.4-sonar/",
	"title": "Create ECS Cluster",
	"tags": [],
	"description": "",
	"content": "Create ECS Cluster Create ecs.tf with the configuration below:\nresource \u0026#34;aws_ecs_cluster\u0026#34; \u0026#34;main\u0026#34; { name = var.ecs_cluster_name tags = { Name = var.ecs_cluster_name } } The name of the ECS Cluster is the variable var.ecs_cluster_name. Follow this link for checking all defined variables.\nCreate Task Definition Next, we will create Task Definition. In ecs.tf, adding the configuration as below:\nresource \u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;my_ECS_TD\u0026#34; { family = \u0026#34;ECS_TaskDefinition\u0026#34; container_definitions = jsonencode([ { name = var.container_name image = var.image_name cpu = 500 memory = 500 essential = true portMappings = [ { containerPort = var.container_port hostPort = 0 protocol = \u0026#34;tcp\u0026#34; } ] } ]) } Our Task Definition contains a container, which is launched from a Docker Image from Docker Hub. This container exposes port var.container_port (3000). When a container running on an EC2 Cluster, it will bind the port of the container to the one of ports in range 49153-65535 or 32768-61000 on the EC2 Cluster. And the most important is we can allocate resources for the container:\n0.5 vCPU for the task (This task contains only one container) 500 MiB of memory for this task (This task contains only one container container) So, we solved the problem of allocating computing resources.\nCreate ECS Service Next, we will crete ECS Service (Service will be responsible for launching task). But first, we need to create IAM Role for Service to allow them interacting with EC2 Cluster and Load Balancer. In iamrole.tf, adding the configuration as below:\n# iamrole.tf data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;ecs_service_policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRole\u0026#34;] effect = \u0026#34;Allow\u0026#34; principals { type = \u0026#34;Service\u0026#34; identifiers = [\u0026#34;ecs.amazonaws.com\u0026#34;,] } } } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;ecs_service_role\u0026#34; { name = \u0026#34;ECS_ServiceRole\u0026#34; assume_role_policy = data.aws_iam_policy_document.ecs_service_policy.json } resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;ecs_service_role_policy\u0026#34; { name = \u0026#34;ECS_ServiceRolePolicy\u0026#34; policy = data.aws_iam_policy_document.ecs_service_role_policy.json role = aws_iam_role.ecs_service_role.id } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;ecs_service_role_policy\u0026#34; { statement { effect = \u0026#34;Allow\u0026#34; actions = [ \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;elasticloadbalancing:DeregisterInstancesFromLoadBalancer\u0026#34;, \u0026#34;elasticloadbalancing:DeregisterTargets\u0026#34;, \u0026#34;elasticloadbalancing:Describe*\u0026#34;, \u0026#34;elasticloadbalancing:RegisterInstancesWithLoadBalancer\u0026#34;, \u0026#34;elasticloadbalancing:RegisterTargets\u0026#34;, \u0026#34;ec2:DescribeTags\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:PutSubscriptionFilter\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ] resources = [\u0026#34;*\u0026#34;] } } Finally, we will create ECS Service. In ecs.tf, adding the configuration as below:\nresource \u0026#34;aws_ecs_service\u0026#34; \u0026#34;service\u0026#34; { name = \u0026#34;ECS_Service\u0026#34; iam_role = aws_iam_role.ecs_service_role.arn cluster = aws_ecs_cluster.main.id task_definition = aws_ecs_task_definition.my_ECS_TD.arn desired_count = var.ecs_task_desired_count load_balancer { target_group_arn = aws_alb_target_group.alb_target_group.arn container_name = var.container_name container_port = var.container_port } ## Spread tasks evenly accross all Availability Zones for High Availability ordered_placement_strategy { type = \u0026#34;spread\u0026#34; field = \u0026#34;attribute:ecs.availability-zone\u0026#34; } ## Make use of all available space on the Container Instances ordered_placement_strategy { type = \u0026#34;binpack\u0026#34; field = \u0026#34;memory\u0026#34; } # Do not update desired count again to avoid a reset to this number on every deploymengit t lifecycle { ignore_changes = [desired_count] } } With the above configuration, we have:\nECS Service belonging to the defined ECS Cluster IAM Role for ECS Service Task Definition in ECS Service Task (Container(s)) will be launched into EC2 Cluster and registered into the Target Group. So, Load Balancer can forward traffic to them. Task will be distributed across Availability Zones to ensure the High Availability In case of increasing the number of desired task, ECS Service guarantees using all available space on the EC2 Cluster. Now, we have a completed AWS Infrastructure.\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/2.2.5-run-terraform/",
	"title": "Create ECS Cluster",
	"tags": [],
	"description": "",
	"content": "Create ECS Cluster Create ecs.tf with the configuration below:\nresource \u0026#34;aws_ecs_cluster\u0026#34; \u0026#34;main\u0026#34; { name = var.ecs_cluster_name tags = { Name = var.ecs_cluster_name } } The name of the ECS Cluster is the variable var.ecs_cluster_name. Follow this link for checking all defined variables.\nCreate Task Definition Next, we will create Task Definition. In ecs.tf, adding the configuration as below:\nresource \u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;my_ECS_TD\u0026#34; { family = \u0026#34;ECS_TaskDefinition\u0026#34; container_definitions = jsonencode([ { name = var.container_name image = var.image_name cpu = 500 memory = 500 essential = true portMappings = [ { containerPort = var.container_port hostPort = 0 protocol = \u0026#34;tcp\u0026#34; } ] } ]) } Our Task Definition contains a container, which is launched from a Docker Image from Docker Hub. This container exposes port var.container_port (3000). When a container running on an EC2 Cluster, it will bind the port of the container to the one of ports in range 49153-65535 or 32768-61000 on the EC2 Cluster. And the most important is we can allocate resources for the container:\n0.5 vCPU for the task (This task contains only one container) 500 MiB of memory for this task (This task contains only one container container) So, we solved the problem of allocating computing resources.\nCreate ECS Service Next, we will crete ECS Service (Service will be responsible for launching task). But first, we need to create IAM Role for Service to allow them interacting with EC2 Cluster and Load Balancer. In iamrole.tf, adding the configuration as below:\n# iamrole.tf data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;ecs_service_policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRole\u0026#34;] effect = \u0026#34;Allow\u0026#34; principals { type = \u0026#34;Service\u0026#34; identifiers = [\u0026#34;ecs.amazonaws.com\u0026#34;,] } } } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;ecs_service_role\u0026#34; { name = \u0026#34;ECS_ServiceRole\u0026#34; assume_role_policy = data.aws_iam_policy_document.ecs_service_policy.json } resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;ecs_service_role_policy\u0026#34; { name = \u0026#34;ECS_ServiceRolePolicy\u0026#34; policy = data.aws_iam_policy_document.ecs_service_role_policy.json role = aws_iam_role.ecs_service_role.id } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;ecs_service_role_policy\u0026#34; { statement { effect = \u0026#34;Allow\u0026#34; actions = [ \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;elasticloadbalancing:DeregisterInstancesFromLoadBalancer\u0026#34;, \u0026#34;elasticloadbalancing:DeregisterTargets\u0026#34;, \u0026#34;elasticloadbalancing:Describe*\u0026#34;, \u0026#34;elasticloadbalancing:RegisterInstancesWithLoadBalancer\u0026#34;, \u0026#34;elasticloadbalancing:RegisterTargets\u0026#34;, \u0026#34;ec2:DescribeTags\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:PutSubscriptionFilter\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ] resources = [\u0026#34;*\u0026#34;] } } Finally, we will create ECS Service. In ecs.tf, adding the configuration as below:\nresource \u0026#34;aws_ecs_service\u0026#34; \u0026#34;service\u0026#34; { name = \u0026#34;ECS_Service\u0026#34; iam_role = aws_iam_role.ecs_service_role.arn cluster = aws_ecs_cluster.main.id task_definition = aws_ecs_task_definition.my_ECS_TD.arn desired_count = var.ecs_task_desired_count load_balancer { target_group_arn = aws_alb_target_group.alb_target_group.arn container_name = var.container_name container_port = var.container_port } ## Spread tasks evenly accross all Availability Zones for High Availability ordered_placement_strategy { type = \u0026#34;spread\u0026#34; field = \u0026#34;attribute:ecs.availability-zone\u0026#34; } ## Make use of all available space on the Container Instances ordered_placement_strategy { type = \u0026#34;binpack\u0026#34; field = \u0026#34;memory\u0026#34; } # Do not update desired count again to avoid a reset to this number on every deploymengit t lifecycle { ignore_changes = [desired_count] } } With the above configuration, we have:\nECS Service belonging to the defined ECS Cluster IAM Role for ECS Service Task Definition in ECS Service Task (Container(s)) will be launched into EC2 Cluster and registered into the Target Group. So, Load Balancer can forward traffic to them. Task will be distributed across Availability Zones to ensure the High Availability In case of increasing the number of desired task, ECS Service guarantees using all available space on the EC2 Cluster. Now, we have a completed AWS Infrastructure.\n"
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]