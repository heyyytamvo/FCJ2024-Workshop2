[
{
	"uri": "//localhost:1313/",
	"title": "A Secured CI/CD Pipeline by implementing DevSecOps practice and GitOps principal for a Microservices Application on AWS EKS",
	"tags": [],
	"description": "",
	"content": "A Secured CI/CD Pipeline by implementing DevSecOps practice and GitOps principal for a Microservices Application on AWS EKS Overview In this workshop, we will integrate DevSecOps Practice to a CI/CD for an application running on Amazon Elastic Kubernetes Service (EKS). To leverage our DevOps practice, we will apply the GitOps Principle during this workshop. You will begin with provisioning the AWS Infrastructure by Terraform, building a CI/CD Pipeline, and finally, you will integrate the DevSecOps practice to secure our software development life cycle.\nExpectation Cloud Infrastructure Below is the final AWS Infrastructure and all the traffic after finishing this workshop.\nCI Pipeline CI Pipeline is described below.\nCD Pipeline CD Pipeline is described below\nFor your preference, below are the repositories.\nDev Repository (where Dev team work): link Ops Repository (Where Operations team work): link Content Problem Definition Preparation Scaling Testing Cleanup "
},
{
	"uri": "//localhost:1313/3-eks/3.1-eks-role/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/3-eks/3.3-eks-create/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/3-eks/3.4-eks-worker-node-create/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/3-eks/3.5-ecr/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/3-eks/3.6-run-terraform/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.2-jenkins/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.3-dev-repo/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.4-argocd-autodeploy/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.5-prometheus-grafana/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.6-efk/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.7-bashscript/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.8-k8s/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/5-finish-monitoring/5.2-efk/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/6-devsecops/6.2-sonar/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/6-devsecops/6.3-jenkins/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/6-devsecops/6.4-trivy-owasp-zap/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/7-argocd-autodeploy/7.2-ops-repo-update/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/7-argocd-autodeploy/7.3-argocd/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/7-argocd-autodeploy/7.4-app-access/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/8-cicd-test/8.1-change-code/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/2.2.1-key/",
	"title": "Create Key Pair at Local Machine",
	"tags": [],
	"description": "",
	"content": "Create Public and Private Key at your local machine Using ssh-keygen to create a key pair and name it EC2.\nssh-keygen -b 2048 -t rsa Please add it to .gitignore to avoid leaking credentials.\nCreate Terraform/04-keypair.tf with the configuration below:\n## Key pair resource \u0026#34;aws_key_pair\u0026#34; \u0026#34;EC2key\u0026#34; { key_name = var.ec2_key_name public_key = file(\u0026#34;${path.module}/EC2.pub\u0026#34;) } "
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.1-vpc/",
	"title": "Create VPC and Bastion Host",
	"tags": [],
	"description": "",
	"content": "Content Create VPC, Internet Gateway, và NAT Gateway Create Subnet and Subnet Association Run Terraform After finishing this section, below is the architecture:\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.1-vpc/2.1.1-createvpc/",
	"title": "Create VPC, Internet Gateway, and NAT Gateway",
	"tags": [],
	"description": "",
	"content": "Creating a Folder Terraform, then, create files Terraform/terraform.tfvars and Terraform/variables.tf. These file will define our Infrastructure Configuration.\nPlease refer to this file Terraform/terraform.tfvars and Terraform/variables.tf\nCreate file Terraform/00-main.tf to define AWS Provider.\nterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } } required_version = \u0026#34;\u0026gt;= 1.2.0\u0026#34; } provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; } Create VPC, Internet Gateway, and NAT Gateway Create file Terraform/01-vpc.tf to create VPC, Internet Gateway, and NAT Gateway.\nresource \u0026#34;aws_vpc\u0026#34; \u0026#34;main\u0026#34; { cidr_block = var.vpc_cidr enable_dns_hostnames = true enable_dns_support = true tags = { Name = var.vpc_name } } resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;defaultIGW\u0026#34; { vpc_id = aws_vpc.main.id tags = { Name = \u0026#34;Internet Gateway\u0026#34; } } resource \u0026#34;aws_eip\u0026#34; \u0026#34;my_elastic_ip\u0026#34; { domain = \u0026#34;vpc\u0026#34; } # NAT Gateway resource \u0026#34;aws_nat_gateway\u0026#34; \u0026#34;my_nat_gtw\u0026#34; { allocation_id = aws_eip.my_elastic_ip.id subnet_id = values(aws_subnet.public_subnets)[0].id tags = { Name = \u0026#34;NAT Gateway\u0026#34; } } We created an Elastic IP Address and attach it to the NAT Gateway\n"
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Problem Definition",
	"tags": [],
	"description": "",
	"content": "Why Microservices? Before diving into Microservices, we will get into the traditional software architecture in the deployment stage: Monolithic. Monolithic is a software architecture in which all modules (features/services) are containerized in a single ‘block’. The most common containerization technology that you may have heard of is Docker. Supposedly your software has already been packaged into a ‘block’ and deployed to the Virtual Private Server (VPS). Now, your development team has released a new feature. Then, your application needs to be updated. With monolithic architecture, the software updating process is demonstrated below:\nAs you can see, you need to shut down the application for a while. Then, a new version will be deployed. In the meantime, end users can not access the application. With business consideration, this is a huge issue: updating a feature affects the entire application. Of course, we can solve the problem by implementing the blue/green deployment strategy, but it is out of scope in this workshop.\nThere is another software architecture that can handle this problem: Microservices. Microservices apply the theory of divide and conquer: Every service (feature) of the application will be packaged as a single \u0026lsquo;block\u0026rsquo; (or Image) and they communicate to each other by some standards such as REST, v.v. Updating a feature has no impact on other features as described below:\nKubernetes (K8s) is the most common tool for deploying a microservice application because of its utilization in allocating computing resources. In this workshop, we will use Kubernetes to deploy a simple microservices application. However, we will not set up the entire K8s Cluster. Instead, we will use Elastic Kubernetes Service (EKS) from AWS. The infrastructure deployment stage requires a smooth collaboration between the Operations Team (or Ops Team including System Engineer, System Administration, etc.). Hence, GitOps is the key point to guarantee that collaboration.\nWhy GitOps? Why DevSecOps? DevOps practice enhances an organization’s ability to deliver applications by using automation tools. Hence, the amount of required time to release a new feature will be effectively reduced, then:\nEnd users receive the latest version of the application as soon as possible The development team will focus on their tasks since other manual work (building, testing, deploying, etc.) is automated During the automation process, could we find out some security risks from the source code, built image, or even our production? DevSecOps is the answer to that question. In this workshop, we will integrate some simple DevSecOps techniques such as:\nSAST (Static Application Security Testing): A type of security testing that analyzes our code base before executing the program. Image Scan and Secure IaC: Scan and detect the vulnerabilities in the built image before deploying it to the production environment. Scan our infrastructure based on the IaC code base. DAST (Dynamic Application Security Testing): While SAST analyzes static code, DAST performs security testing while the application is operating. Monitoring Suppose you are a developer and our application is deployed in a Kubernetes (K8s) Cluster. One day, the application crashed and you need to fix it as soon as possible. The first thing that comes to your mind is checking the logs. However, you have no idea how to access the K8s Cluster because it is too complicated. Hence, we need to centralize all the application logs in one place. In this workshop, we will deploy a EFK Stack to solve this problem. After the EFK is successfully deployed, developers will use their web browser to read all the application logs. Below is the final practice.\nIn this workshop, we will use these services from Amazon Web Service to solve all the problems above:\nElastic Compute Service (EC2): Jenkins Server, and SonarQube Server will be hosted here. They played an essential part in our CI/CD Pipeline Elastic Kubernetes Service (EKS): Instead of setting up a Kubernetes Cluster, we will use service from AWS Elastic Container Registry (ECR): It serves as a “private Docker Registry” where we store our application image. You need to install Terraform to finish this Workshop\n"
},
{
	"uri": "//localhost:1313/8-cicd-test/8.2-ci-sec/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/8-cicd-test/8.3-change-code-ops/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/8-cicd-test/8.4-cd-sec/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/8-cicd-test/8.5-app/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/2.2.2-jump-host/",
	"title": "Create Jump Host",
	"tags": [],
	"description": "",
	"content": "We will manipulate the Kubernetes Cluster via Jump Host. This implementation will reduce the chance of unauthorized accessing the Kubernetes Cluster. Using the Private Key that we created in the previous part, Ops Team can access the Jump Host via SSH Protocol. But first, we need to prepare all necessary packages by using Bash Script. Creating file Terraform/jump_host_install.sh as below:\n#!/bin/bash sudo apt update -y # Install eksctl ARCH=amd64 PLATFORM=$(uname -s)_$ARCH curl -sLO \u0026#34;https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz\u0026#34; # (Optional) Verify checksum curl -sL \u0026#34;https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt\u0026#34; | grep $PLATFORM | sha256sum --check sudo tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp \u0026amp;\u0026amp; rm eksctl_$PLATFORM.tar.gz sudo mv /tmp/eksctl /usr/local/bin # Install kubectl curl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; curl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\u0026#34; sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl # install aws-cli sudo apt install -y unzip curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update # install docker sudo apt-get install -y ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;) stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin sudo curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 sudo chmod 700 get_helm.sh sudo bash get_helm.sh Next, we will create the configuration for the Jump Host. Create file Terraform/05-jump_host.tf as below:\n# Security Group for EC2 resource \u0026#34;aws_security_group\u0026#34; \u0026#34;JUMP_HOST_SG\u0026#34; { name = \u0026#34;JUMP_HOST_SG\u0026#34; description = \u0026#34;Allow SSH inbound and all outbound traffic\u0026#34; vpc_id = aws_vpc.main.id ingress = [ { from_port = -1 to_port = -1 protocol = \u0026#34;icmp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;], description = \u0026#34;Allow inbound ICMP Traffic\u0026#34; ipv6_cidr_blocks = [] prefix_list_ids = [] security_groups = [] self = false }, { from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] description = \u0026#34;Allow inbound traffic on port 22\u0026#34; ipv6_cidr_blocks = [] prefix_list_ids = [] security_groups = [] self = false } ] egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = { Name = \u0026#34;JUMP_HOST SG\u0026#34; } } resource \u0026#34;aws_instance\u0026#34; \u0026#34;Jump_host\u0026#34; { ami = var.ec2_ami instance_type = var.ec2_instance_type key_name = aws_key_pair.EC2key.key_name monitoring = true subnet_id = values(aws_subnet.public_subnets)[0].id vpc_security_group_ids = [aws_security_group.JUMP_HOST_SG.id] associate_public_ip_address = true user_data = file(\u0026#34;${path.module}/jump_host_install.sh\u0026#34;) tags = { Terraform = \u0026#34;true\u0026#34; Environment = \u0026#34;dev\u0026#34; Name = \u0026#34;Jump Host\u0026#34; } root_block_device { volume_size = 30 } } "
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/",
	"title": "Create Jump Host, Jenkins, and Sonarqube Server",
	"tags": [],
	"description": "",
	"content": "Nội dung Create Public and Private Key at Local Machine Create Jump Host Create Jenkins Server Create SonarQube Server Run Terraform Sau khi hoàn thành phần này, hình bên dưới sẽ là hạ tầng của chúng ta:\nAfter finishing this section, below is the architecture:\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.1-vpc/2.1.2-create-subnet/",
	"title": "Create Subnet and Subnet Association",
	"tags": [],
	"description": "",
	"content": "Create Subnet Create file Terraform/02-subnet.tf to create subnets as below:\n// Public Subnet resource \u0026#34;aws_subnet\u0026#34; \u0026#34;public_subnets\u0026#34; { for_each = toset(var.vpc_public_subnets) vpc_id = aws_vpc.main.id cidr_block = each.value availability_zone = var.vpc_azs[index(var.vpc_public_subnets, each.value)] map_public_ip_on_launch = true tags = { Name = \u0026#34;Public Subnet ${index(var.vpc_public_subnets, each.value) + 1}\u0026#34; } } // Private Subnet resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private_subnets\u0026#34; { for_each = toset(var.vpc_private_subnets) vpc_id = aws_vpc.main.id cidr_block = each.value availability_zone = var.vpc_azs[index(var.vpc_private_subnets, each.value)] tags = { Name = \u0026#34;Private Subnet ${index(var.vpc_private_subnets, each.value) + 1}\u0026#34; } } Route Table and Subnet Association Next, we will create Route Tables, then, we will associate Internet Gateway, NAT Gateway, and Subnets. Create file Terraform/03-route_table.tf as below:\n// create public route table resource \u0026#34;aws_route_table\u0026#34; \u0026#34;public_route_table\u0026#34; { vpc_id = aws_vpc.main.id route { cidr_block = var.vpc_cidr gateway_id = \u0026#34;local\u0026#34; } route { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; gateway_id = aws_internet_gateway.defaultIGW.id } tags = { Name = \u0026#34;Public Route Table\u0026#34; } } // create private route table resource \u0026#34;aws_route_table\u0026#34; \u0026#34;private_route_table\u0026#34; { vpc_id = aws_vpc.main.id tags = { Name = \u0026#34;Private Route Table\u0026#34; } } ## public subnet Association resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;public_subnets_associations\u0026#34; { for_each = aws_subnet.public_subnets subnet_id = each.value.id route_table_id = aws_route_table.public_route_table.id } ## private subnet Association resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;private_subnets_associations\u0026#34; { for_each = aws_subnet.private_subnets subnet_id = each.value.id route_table_id = aws_route_table.private_route_table.id } resource \u0026#34;aws_route\u0026#34; \u0026#34;route_nat_gw\u0026#34; { route_table_id = aws_route_table.private_route_table.id destination_cidr_block = \u0026#34;0.0.0.0/0\u0026#34; nat_gateway_id = aws_nat_gateway.my_nat_gtw.id timeouts { create = \u0026#34;5m\u0026#34; } } "
},
{
	"uri": "//localhost:1313/2-vpc-ec2/",
	"title": "Deploy VPC, Jenkins, and SonarQube Server",
	"tags": [],
	"description": "",
	"content": "\rWe use Ops Repo in this section\nTrong phần này, chúng ta sẽ triển khai một VPC với:\n3 Availability Zones 3 Public Subnets 3 Private Subnets 1 Internet Gateway 1 NAT Gateway 3 EC2, where: 1 Jump Host for interacting with the Kubernetes Cluster 1 Jenkins Server for CI pipeline 1 SonarQube Server for DevSecOps Pipeline Content Create VPC Create Jump Host, Jenkins Server, and SonarQube Server "
},
{
	"uri": "//localhost:1313/3-eks/3.2-eks-worker-role/",
	"title": "Validate Scaling Ability",
	"tags": [],
	"description": "",
	"content": "Content: Send 100.000 requests to Load Balancer Send 1.000.000 requests to Load Balancer Scale up Desired Task "
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/2.2.3-jenkins/",
	"title": "Create Jenkins Server",
	"tags": [],
	"description": "",
	"content": "All CI Pipeline will be executed on the Jenins Server. Everytime we have a new commit at Dev Repo or Ops Repo, Jenkins will automatically execute all pipelines depending on the configuration.\nCreate file Terraform/jenkins_install.sh as below to install packages for Jenkins Server.\n#!/bin/bash sudo apt update -y # install docker sudo apt-get install -y ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;) stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # install aws-cli sudo apt install -y unzip curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update # install java and jenkins sudo apt install -y fontconfig openjdk-17-jre sudo wget -O /usr/share/keyrings/jenkins-keyring.asc \\ https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key echo \u0026#34;deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc]\u0026#34; \\ https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ /etc/apt/sources.list.d/jenkins.list \u0026gt; /dev/null sudo apt-get update sudo apt-get install -y jenkins sudo systemctl start jenkins # install nginx sudo apt install -y nginx sudo systemctl start nginx sudo rm /etc/nginx/sites-enabled/default sudo tee /etc/nginx/sites-enabled/default \u0026gt; /dev/null \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server { listen 80; server_name _; location / { proxy_pass http://localhost:8080; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_redirect off; } } EOF sudo systemctl reload nginx sudo usermod -aG docker jenkins sudo systemctl restart jenkins sudo systemctl restart docker sudo apt-get install -y wget apt-transport-https gnupg wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | gpg --dearmor | sudo tee /usr/share/keyrings/trivy.gpg \u0026gt; /dev/null echo \u0026#34;deb [signed-by=/usr/share/keyrings/trivy.gpg] https://aquasecurity.github.io/trivy-repo/deb generic main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/trivy.list sudo apt-get update sudo apt-get install -y trivy # Install OWASP ZAP docker pull zaproxy/zap-stable Next, we will configure the Jenkins Server. Create file Terraform/05-jenkins_server.tf as below:\n# Security Group for EC2\rresource \u0026#34;aws_security_group\u0026#34; \u0026#34;JENKINS_SG\u0026#34; {\rname = \u0026#34;JENKINS_SG\u0026#34;\rdescription = \u0026#34;Allow HTTP inbound and all outbound traffic\u0026#34;\rvpc_id = aws_vpc.main.id\ringress = [\r{\rfrom_port = 80\rto_port = 80\rprotocol = \u0026#34;tcp\u0026#34;\rcidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;]\rdescription = \u0026#34;Allow inbound traffic on port 80\u0026#34;\ripv6_cidr_blocks = []\rprefix_list_ids = []\rsecurity_groups = []\rself = false\r},\r{\rfrom_port = -1\rto_port = -1\rprotocol = \u0026#34;icmp\u0026#34;\rcidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;],\rdescription = \u0026#34;Allow inbound ICMP Traffic\u0026#34;\ripv6_cidr_blocks = []\rprefix_list_ids = []\rsecurity_groups = []\rself = false\r},\r{\rfrom_port = 22\rto_port = 22\rprotocol = \u0026#34;tcp\u0026#34;\rcidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;]\rdescription = \u0026#34;Allow inbound traffic on port 22\u0026#34;\ripv6_cidr_blocks = []\rprefix_list_ids = []\rsecurity_groups = []\rself = false\r}\r]\regress {\rfrom_port = 0\rto_port = 0\rprotocol = \u0026#34;-1\u0026#34;\rcidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;]\r}\rtags = {\rName = \u0026#34;Jenkins Server SG\u0026#34;\r}\r}\rresource \u0026#34;aws_instance\u0026#34; \u0026#34;Jenkins_server\u0026#34; {\rami = var.ec2_ami\rinstance_type = var.ec2_instance_type\rkey_name = aws_key_pair.EC2key.key_name\rmonitoring = true\rsubnet_id = values(aws_subnet.public_subnets)[2].id\rvpc_security_group_ids = [aws_security_group.JENKINS_SG.id]\rassociate_public_ip_address = true\ruser_data = file(\u0026#34;${path.module}/jenkins_install.sh\u0026#34;)\rtags = {\rTerraform = \u0026#34;true\u0026#34;\rEnvironment = \u0026#34;dev\u0026#34;\rName = \u0026#34;Jenkins Server\u0026#34;\r}\rroot_block_device {\rvolume_size = 30\r}\r} "
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.1-vpc/2.1.3-run-terraform/",
	"title": "Run Association",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/3-eks/",
	"title": "Validate Scaling",
	"tags": [],
	"description": "",
	"content": "In this step, we will validate the scalability of the infrastructure.\nContent Connect to Bastion Host Validate Scaling Ability "
},
{
	"uri": "//localhost:1313/4-cicd/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Using Terminal and executing this command lines:\nterraform destroy Now, all resources will be removed as below:\n"
},
{
	"uri": "//localhost:1313/5-finish-monitoring/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Using Terminal and executing this command lines:\nterraform destroy Now, all resources will be removed as below:\n"
},
{
	"uri": "//localhost:1313/6-devsecops/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Using Terminal and executing this command lines:\nterraform destroy Now, all resources will be removed as below:\n"
},
{
	"uri": "//localhost:1313/7-argocd-autodeploy/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Using Terminal and executing this command lines:\nterraform destroy Now, all resources will be removed as below:\n"
},
{
	"uri": "//localhost:1313/8-cicd-test/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Using Terminal and executing this command lines:\nterraform destroy Now, all resources will be removed as below:\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.1-application/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/5-finish-monitoring/5.1-prome-grafana/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/6-devsecops/6.1-ops-repo/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/7-argocd-autodeploy/7.1-ci-pipeline/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/2.2.5-run-terraform/",
	"title": "Create ECS Cluster",
	"tags": [],
	"description": "",
	"content": "Create ECS Cluster Create ecs.tf with the configuration below:\nresource \u0026#34;aws_ecs_cluster\u0026#34; \u0026#34;main\u0026#34; { name = var.ecs_cluster_name tags = { Name = var.ecs_cluster_name } } The name of the ECS Cluster is the variable var.ecs_cluster_name. Follow this link for checking all defined variables.\nCreate Task Definition Next, we will create Task Definition. In ecs.tf, adding the configuration as below:\nresource \u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;my_ECS_TD\u0026#34; { family = \u0026#34;ECS_TaskDefinition\u0026#34; container_definitions = jsonencode([ { name = var.container_name image = var.image_name cpu = 500 memory = 500 essential = true portMappings = [ { containerPort = var.container_port hostPort = 0 protocol = \u0026#34;tcp\u0026#34; } ] } ]) } Our Task Definition contains a container, which is launched from a Docker Image from Docker Hub. This container exposes port var.container_port (3000). When a container running on an EC2 Cluster, it will bind the port of the container to the one of ports in range 49153-65535 or 32768-61000 on the EC2 Cluster. And the most important is we can allocate resources for the container:\n0.5 vCPU for the task (This task contains only one container) 500 MiB of memory for this task (This task contains only one container container) So, we solved the problem of allocating computing resources.\nCreate ECS Service Next, we will crete ECS Service (Service will be responsible for launching task). But first, we need to create IAM Role for Service to allow them interacting with EC2 Cluster and Load Balancer. In iamrole.tf, adding the configuration as below:\n# iamrole.tf data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;ecs_service_policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRole\u0026#34;] effect = \u0026#34;Allow\u0026#34; principals { type = \u0026#34;Service\u0026#34; identifiers = [\u0026#34;ecs.amazonaws.com\u0026#34;,] } } } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;ecs_service_role\u0026#34; { name = \u0026#34;ECS_ServiceRole\u0026#34; assume_role_policy = data.aws_iam_policy_document.ecs_service_policy.json } resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;ecs_service_role_policy\u0026#34; { name = \u0026#34;ECS_ServiceRolePolicy\u0026#34; policy = data.aws_iam_policy_document.ecs_service_role_policy.json role = aws_iam_role.ecs_service_role.id } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;ecs_service_role_policy\u0026#34; { statement { effect = \u0026#34;Allow\u0026#34; actions = [ \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;elasticloadbalancing:DeregisterInstancesFromLoadBalancer\u0026#34;, \u0026#34;elasticloadbalancing:DeregisterTargets\u0026#34;, \u0026#34;elasticloadbalancing:Describe*\u0026#34;, \u0026#34;elasticloadbalancing:RegisterInstancesWithLoadBalancer\u0026#34;, \u0026#34;elasticloadbalancing:RegisterTargets\u0026#34;, \u0026#34;ec2:DescribeTags\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:PutSubscriptionFilter\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ] resources = [\u0026#34;*\u0026#34;] } } Finally, we will create ECS Service. In ecs.tf, adding the configuration as below:\nresource \u0026#34;aws_ecs_service\u0026#34; \u0026#34;service\u0026#34; { name = \u0026#34;ECS_Service\u0026#34; iam_role = aws_iam_role.ecs_service_role.arn cluster = aws_ecs_cluster.main.id task_definition = aws_ecs_task_definition.my_ECS_TD.arn desired_count = var.ecs_task_desired_count load_balancer { target_group_arn = aws_alb_target_group.alb_target_group.arn container_name = var.container_name container_port = var.container_port } ## Spread tasks evenly accross all Availability Zones for High Availability ordered_placement_strategy { type = \u0026#34;spread\u0026#34; field = \u0026#34;attribute:ecs.availability-zone\u0026#34; } ## Make use of all available space on the Container Instances ordered_placement_strategy { type = \u0026#34;binpack\u0026#34; field = \u0026#34;memory\u0026#34; } # Do not update desired count again to avoid a reset to this number on every deploymengit t lifecycle { ignore_changes = [desired_count] } } With the above configuration, we have:\nECS Service belonging to the defined ECS Cluster IAM Role for ECS Service Task Definition in ECS Service Task (Container(s)) will be launched into EC2 Cluster and registered into the Target Group. So, Load Balancer can forward traffic to them. Task will be distributed across Availability Zones to ensure the High Availability In case of increasing the number of desired task, ECS Service guarantees using all available space on the EC2 Cluster. Now, we have a completed AWS Infrastructure.\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/2.2.4-sonar/",
	"title": "Create SonarQube Server",
	"tags": [],
	"description": "",
	"content": "Our code base will analyzed by the SonarQube Server. You can deploy Jenkins and SonarQube Server on a single EC2 for saving budget. In this Workshop, I will use two EC2 Instances. Create file Terraform/sonarqube_install.sh as below:\n#!/bin/bash sudo apt update -y sudo apt install -y fontconfig openjdk-17-jre # Sonarqube sudo apt install curl ca-certificates sudo install -d /usr/share/postgresql-common/pgdg sudo curl -o /usr/share/postgresql-common/pgdg/apt.postgresql.org.asc --fail https://www.postgresql.org/media/keys/ACCC4CF8.asc sudo sh -c \u0026#39;echo \u0026#34;deb [signed-by=/usr/share/postgresql-common/pgdg/apt.postgresql.org.asc] https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\u0026#34; \u0026gt; /etc/apt/sources.list.d/pgdg.list\u0026#39; sudo apt update sudo apt install postgresql-15 -y sudo -i -u postgres bash \u0026lt;\u0026lt; EOF # Create the user and database, and set the password createuser sonar createdb sonar -O sonar psql -c \u0026#34;ALTER USER sonar WITH ENCRYPTED PASSWORD \u0026#39;your_password\u0026#39;;\u0026#34; EOF wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-10.5.1.90531.zip sudo apt install -y unzip sudo unzip sonarqube-10.5.1.90531.zip sudo mv sonarqube-10.5.1.90531 /opt/sonarqube sudo adduser --system --no-create-home --group --disabled-login sonarqube sudo chown -R sonarqube:sonarqube /opt/sonarqube sudo bash -c \u0026#39;cat \u0026lt;\u0026lt; EOF \u0026gt; /opt/sonarqube/conf/sonar.properties sonar.jdbc.username=sonar sonar.jdbc.password=your_password sonar.jdbc.url=jdbc:postgresql://localhost/sonar EOF\u0026#39; sudo bash -c \u0026#39;cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/systemd/system/sonarqube.service [Unit] Description=SonarQube service After=syslog.target network.target [Service] Type=forking ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop User=sonarqube Group=sonarqube Restart=always LimitNOFILE=65536 LimitNPROC=4096 [Install] WantedBy=multi-user.target EOF\u0026#39; sudo systemctl daemon-reload sudo systemctl start sonarqube sudo systemctl enable sonarqube # Update limits.conf sudo bash -c \u0026#39;cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/security/limits.conf sonarqube - nofile 65536 sonarqube - nproc 4096 EOF\u0026#39; # Update sysctl.conf sudo bash -c \u0026#39;cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/sysctl.conf vm.max_map_count=262144 EOF\u0026#39; sudo sysctl -p # install nginx sudo apt install -y nginx sudo systemctl start nginx sudo rm /etc/nginx/sites-enabled/default sudo tee /etc/nginx/sites-enabled/default \u0026gt; /dev/null \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server { listen 80; server_name _; location / { proxy_pass http://localhost:9000; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_redirect off; } } EOF sudo systemctl reload nginx Next, we will configure our Sonar Server. Creating file Terraform/05-sonar_server.tf as below:\n# Security Group for EC2 resource \u0026#34;aws_security_group\u0026#34; \u0026#34;SONAR_HOST_SG\u0026#34; { name = \u0026#34;SONAR_HOST_SG\u0026#34; description = \u0026#34;Allow SSH, HTTP inbound and all outbound traffic\u0026#34; vpc_id = aws_vpc.main.id ingress = [ { from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] description = \u0026#34;Allow inbound traffic on port 80\u0026#34; ipv6_cidr_blocks = [] prefix_list_ids = [] security_groups = [] self = false }, { from_port = -1 to_port = -1 protocol = \u0026#34;icmp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;], description = \u0026#34;Allow inbound ICMP Traffic\u0026#34; ipv6_cidr_blocks = [] prefix_list_ids = [] security_groups = [] self = false }, { from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] description = \u0026#34;Allow inbound traffic on port 22\u0026#34; ipv6_cidr_blocks = [] prefix_list_ids = [] security_groups = [] self = false } ] egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = { Name = \u0026#34;SONAR_HOST SG\u0026#34; } } resource \u0026#34;aws_instance\u0026#34; \u0026#34;Sonar_host\u0026#34; { ami = var.ec2_ami instance_type = var.ec2_instance_type key_name = aws_key_pair.EC2key.key_name monitoring = true subnet_id = values(aws_subnet.public_subnets)[1].id vpc_security_group_ids = [aws_security_group.SONAR_HOST_SG.id] associate_public_ip_address = true user_data = file(\u0026#34;${path.module}/sonarqube_install.sh\u0026#34;) tags = { Terraform = \u0026#34;true\u0026#34; Environment = \u0026#34;dev\u0026#34; Name = \u0026#34;Sonar Host\u0026#34; } root_block_device { volume_size = 30 } } "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]