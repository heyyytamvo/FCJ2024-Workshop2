[
{
	"uri": "//localhost:1313/",
	"title": "A Secured CI/CD Pipeline by implementing DevSecOps practice and GitOps principal for a Microservices Application on AWS EKS",
	"tags": [],
	"description": "",
	"content": "A Secured CI/CD Pipeline by implementing DevSecOps practice and GitOps principal for a Microservices Application on AWS EKS Overview In this workshop, we will integrate DevSecOps Practice to a CI/CD for an application running on Amazon Elastic Kubernetes Service (EKS). To leverage our DevOps practice, we will apply the GitOps Principle during this workshop. You will begin with provisioning the AWS Infrastructure by Terraform, building a CI/CD Pipeline, and finally, you will integrate the DevSecOps practice to secure our software development life cycle.\nExpectation Cloud Infrastructure Below is the final AWS Infrastructure and all the traffic after finishing this workshop.\nCI Pipeline CI Pipeline is described below.\nCD Pipeline CD Pipeline is described below\nFor your preference, below are the repositories.\nDev Repository (where Dev team work): link Ops Repository (Where Operations team work): link Content Problem Definition Preparation Scaling Testing Cleanup "
},
{
	"uri": "//localhost:1313/3-eks/3.1-eks-role/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/3-eks/3.3-eks-create/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/3-eks/3.4-eks-worker-node-create/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/3-eks/3.5-ecr/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/3-eks/3.6-run-terraform/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.2-jenkins/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.3-dev-repo/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.4-argocd-autodeploy/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.5-prometheus-grafana/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.6-efk/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.7-bashscript/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.8-k8s/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/5-finish-monitoring/5.2-efk/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/6-devsecops/6.2-sonar/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/6-devsecops/6.3-jenkins/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/6-devsecops/6.4-trivy-owasp-zap/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/7-argocd-autodeploy/7.2-ops-repo-update/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/7-argocd-autodeploy/7.3-argocd/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/7-argocd-autodeploy/7.4-app-access/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/8-cicd-test/8.1-change-code/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/2.2.1-key/",
	"title": "Create Load Balancer and Target Group",
	"tags": [],
	"description": "",
	"content": "Create Load Balancer First, we create Security Group for our Load Balancer. Create alb.tf with the configuration below:\n# Security Group for ALB resource \u0026#34;aws_security_group\u0026#34; \u0026#34;ALB_SG\u0026#34; { name = \u0026#34;ALB SG\u0026#34; description = \u0026#34;Allow HTTP inbound and all outbound traffic\u0026#34; vpc_id = aws_vpc.main.id ingress = [ { from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] description = \u0026#34;Allow inbound traffic on port 80\u0026#34; ipv6_cidr_blocks = [] prefix_list_ids = [] security_groups = [] self = false }, ] egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = { Name = \u0026#34;ALB SG\u0026#34; } } Now, we created Security Group for Load Balancer. This Security Group allows all traffic from port 80 of the Load Balancer.\nNext, we will create the Load Balancer. In file alb.tf, adding these configuration below:\nresource \u0026#34;aws_alb\u0026#34; \u0026#34;alb_vlsm\u0026#34; { name = \u0026#34;ALB-ECS\u0026#34; internal = false load_balancer_type = \u0026#34;application\u0026#34; security_groups = [aws_security_group.ALB_SG.id] subnets = [aws_subnet.public_subnet_1.id, aws_subnet.public_subnet_2.id] # enable_deletion_protection = false tags = { Environment = \u0026#34;production\u0026#34; } } Create Target Group Load Balancer will perform check health on all targets in the Target Group and traffic will be only fowarded to \u0026ldquo;healthy\u0026rdquo; target.\nIn case all targets are considered unhealthy, Load Balancer still forwards traffic to those targets. Please note this!\nIn alb.sg, adding the additional configuration as below:\n## Target Group for our service resource \u0026#34;aws_alb_target_group\u0026#34; \u0026#34;alb_target_group\u0026#34; { name = \u0026#34;TargetGroupForService\u0026#34; port = \u0026#34;80\u0026#34; protocol = \u0026#34;HTTP\u0026#34; vpc_id = aws_vpc.main.id deregistration_delay = 120 health_check { healthy_threshold = \u0026#34;2\u0026#34; unhealthy_threshold = \u0026#34;2\u0026#34; interval = \u0026#34;60\u0026#34; path = \u0026#34;/\u0026#34; port = \u0026#34;traffic-port\u0026#34; protocol = \u0026#34;HTTP\u0026#34; timeout = \u0026#34;30\u0026#34; } } The Load Balancer will send HTTP requests to all targets every 60 seconds. After 2 consecutive successful responses (status 200 by default, depending on how it\u0026rsquo;s configured), the target will be considered Healthy. In contrast, the target is marked as \u0026lsquo;Unhealthy\u0026rsquo; if it fails to respond successfully for 2 consecutive checks.\nConfigure Load Balancer Listener and Listener Rule We need to configure Listener for Load Balancer. Listener is the process of listening traffic on a configured port. For example, Load Balancer will listen on port 80 (for HTTP request) and port 443 (for HTTPS request)\nIn alb.tf, adding the configuration as below:\nresource \u0026#34;aws_alb_listener\u0026#34; \u0026#34;alb_listener_http\u0026#34; { load_balancer_arn = aws_alb.alb_vlsm.arn port = \u0026#34;80\u0026#34; protocol = \u0026#34;HTTP\u0026#34; default_action { type = \u0026#34;fixed-response\u0026#34; fixed_response { content_type = \u0026#34;text/plain\u0026#34; message_body = \u0026#34;Access denied\u0026#34; status_code = \u0026#34;403\u0026#34; } } } resource \u0026#34;aws_alb_listener_rule\u0026#34; \u0026#34;alb_listener_http\u0026#34; { listener_arn = aws_alb_listener.alb_listener_http.arn action { type = \u0026#34;forward\u0026#34; target_group_arn = aws_alb_target_group.alb_target_group.arn } condition { path_pattern { values = [\u0026#34;/*\u0026#34;] } } } "
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.1-vpc/",
	"title": "Create VPC and Bastion Host",
	"tags": [],
	"description": "",
	"content": "Content Create VPC, Internet Gateway, và NAT Gateway Create Subnet and Subnet Association Run Terraform After finishing this section, below is the architecture:\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.1-vpc/2.1.1-createvpc/",
	"title": "Create VPC, Internet Gateway, and NAT Gateway",
	"tags": [],
	"description": "",
	"content": "Creating a Folder Terraform, then, create files Terraform/terraform.tfvars and Terraform/variables.tf. These file will define our Infrastructure Configuration.\nPlease refer to this file Terraform/terraform.tfvars and Terraform/variables.tf\nCreate file Terraform/00-main.tf to define AWS Provider.\nterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } } required_version = \u0026#34;\u0026gt;= 1.2.0\u0026#34; } provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; } Create VPC, Internet Gateway, and NAT Gateway Create file Terraform/01-vpc.tf to create VPC, Internet Gateway, and NAT Gateway.\nresource \u0026#34;aws_vpc\u0026#34; \u0026#34;main\u0026#34; { cidr_block = var.vpc_cidr enable_dns_hostnames = true enable_dns_support = true tags = { Name = var.vpc_name } } resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;defaultIGW\u0026#34; { vpc_id = aws_vpc.main.id tags = { Name = \u0026#34;Internet Gateway\u0026#34; } } resource \u0026#34;aws_eip\u0026#34; \u0026#34;my_elastic_ip\u0026#34; { domain = \u0026#34;vpc\u0026#34; } # NAT Gateway resource \u0026#34;aws_nat_gateway\u0026#34; \u0026#34;my_nat_gtw\u0026#34; { allocation_id = aws_eip.my_elastic_ip.id subnet_id = values(aws_subnet.public_subnets)[0].id tags = { Name = \u0026#34;NAT Gateway\u0026#34; } } We created an Elastic IP Address and attach it to the NAT Gateway\n"
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Problem Definition",
	"tags": [],
	"description": "",
	"content": "Why Microservices? Before diving into Microservices, we will get into the traditional software architecture in the deployment stage: Monolithic. Monolithic is a software architecture in which all modules (features/services) are containerized in a single ‘block’. The most common containerization technology that you may have heard of is Docker. Supposedly your software has already been packaged into a ‘block’ and deployed to the Virtual Private Server (VPS). Now, your development team has released a new feature. Then, your application needs to be updated. With monolithic architecture, the software updating process is demonstrated below:\nAs you can see, you need to shut down the application for a while. Then, a new version will be deployed. In the meantime, end users can not access the application. With business consideration, this is a huge issue: updating a feature affects the entire application. Of course, we can solve the problem by implementing the blue/green deployment strategy, but it is out of scope in this workshop.\nThere is another software architecture that can handle this problem: Microservices. Microservices apply the theory of divide and conquer: Every service (feature) of the application will be packaged as a single \u0026lsquo;block\u0026rsquo; (or Image) and they communicate to each other by some standards such as REST, v.v. Updating a feature has no impact on other features as described below:\nKubernetes (K8s) is the most common tool for deploying a microservice application because of its utilization in allocating computing resources. In this workshop, we will use Kubernetes to deploy a simple microservices application. However, we will not set up the entire K8s Cluster. Instead, we will use Elastic Kubernetes Service (EKS) from AWS. The infrastructure deployment stage requires a smooth collaboration between the Operations Team (or Ops Team including System Engineer, System Administration, etc.). Hence, GitOps is the key point to guarantee that collaboration.\nWhy GitOps? Why DevSecOps? DevOps practice enhances an organization’s ability to deliver applications by using automation tools. Hence, the amount of required time to release a new feature will be effectively reduced, then:\nEnd users receive the latest version of the application as soon as possible The development team will focus on their tasks since other manual work (building, testing, deploying, etc.) is automated During the automation process, could we find out some security risks from the source code, built image, or even our production? DevSecOps is the answer to that question. In this workshop, we will integrate some simple DevSecOps techniques such as:\nSAST (Static Application Security Testing): A type of security testing that analyzes our code base before executing the program. Image Scan and Secure IaC: Scan and detect the vulnerabilities in the built image before deploying it to the production environment. Scan our infrastructure based on the IaC code base. DAST (Dynamic Application Security Testing): While SAST analyzes static code, DAST performs security testing while the application is operating. Monitoring Suppose you are a developer and our application is deployed in a Kubernetes (K8s) Cluster. One day, the application crashed and you need to fix it as soon as possible. The first thing that comes to your mind is checking the logs. However, you have no idea how to access the K8s Cluster because it is too complicated. Hence, we need to centralize all the application logs in one place. In this workshop, we will deploy a EFK Stack to solve this problem. After the EFK is successfully deployed, developers will use their web browser to read all the application logs. Below is the final practice.\nIn this workshop, we will use these services from Amazon Web Service to solve all the problems above:\nElastic Compute Service (EC2): Jenkins Server, and SonarQube Server will be hosted here. They played an essential part in our CI/CD Pipeline Elastic Kubernetes Service (EKS): Instead of setting up a Kubernetes Cluster, we will use service from AWS Elastic Container Registry (ECR): It serves as a “private Docker Registry” where we store our application image. You need to install Terraform to finish this Workshop\n"
},
{
	"uri": "//localhost:1313/8-cicd-test/8.2-ci-sec/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/8-cicd-test/8.3-change-code-ops/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/8-cicd-test/8.4-cd-sec/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/8-cicd-test/8.5-app/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/",
	"title": "Create Load Balancer, Auto Scaling Group, and ECS Cluster",
	"tags": [],
	"description": "",
	"content": "In this section, we will create the entire AWS Infratructure with Load Balancer, Auto Scaling Group, and ECS Cluster.\nContent Create Load Balancer and Target Group Create Template for EC2 belonging to ECS Cluster Create Auto Scaling Group Create ECS Cluster "
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.1-vpc/2.1.2-create-subnet/",
	"title": "Create Subnet and Subnet Association",
	"tags": [],
	"description": "",
	"content": "Create Subnet Create file Terraform/02-subnet.tf to create subnets as below:\n// Public Subnet resource \u0026#34;aws_subnet\u0026#34; \u0026#34;public_subnets\u0026#34; { for_each = toset(var.vpc_public_subnets) vpc_id = aws_vpc.main.id cidr_block = each.value availability_zone = var.vpc_azs[index(var.vpc_public_subnets, each.value)] map_public_ip_on_launch = true tags = { Name = \u0026#34;Public Subnet ${index(var.vpc_public_subnets, each.value) + 1}\u0026#34; } } // Private Subnet resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private_subnets\u0026#34; { for_each = toset(var.vpc_private_subnets) vpc_id = aws_vpc.main.id cidr_block = each.value availability_zone = var.vpc_azs[index(var.vpc_private_subnets, each.value)] tags = { Name = \u0026#34;Private Subnet ${index(var.vpc_private_subnets, each.value) + 1}\u0026#34; } } Route Table and Subnet Association Next, we will create Route Tables, then, we will associate Internet Gateway, NAT Gateway, and Subnets. Create file Terraform/03-route_table.tf as below:\n// create public route table resource \u0026#34;aws_route_table\u0026#34; \u0026#34;public_route_table\u0026#34; { vpc_id = aws_vpc.main.id route { cidr_block = var.vpc_cidr gateway_id = \u0026#34;local\u0026#34; } route { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; gateway_id = aws_internet_gateway.defaultIGW.id } tags = { Name = \u0026#34;Public Route Table\u0026#34; } } // create private route table resource \u0026#34;aws_route_table\u0026#34; \u0026#34;private_route_table\u0026#34; { vpc_id = aws_vpc.main.id tags = { Name = \u0026#34;Private Route Table\u0026#34; } } ## public subnet Association resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;public_subnets_associations\u0026#34; { for_each = aws_subnet.public_subnets subnet_id = each.value.id route_table_id = aws_route_table.public_route_table.id } ## private subnet Association resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;private_subnets_associations\u0026#34; { for_each = aws_subnet.private_subnets subnet_id = each.value.id route_table_id = aws_route_table.private_route_table.id } resource \u0026#34;aws_route\u0026#34; \u0026#34;route_nat_gw\u0026#34; { route_table_id = aws_route_table.private_route_table.id destination_cidr_block = \u0026#34;0.0.0.0/0\u0026#34; nat_gateway_id = aws_nat_gateway.my_nat_gtw.id timeouts { create = \u0026#34;5m\u0026#34; } } "
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/2.2.2-jump-host/",
	"title": "Create Template for EC2 belonging to ECS Cluster",
	"tags": [],
	"description": "",
	"content": "Create Security Group We need to create template for all EC2 belonging to the ECS Cluster. All container will run on these EC2 Instance. From now, let\u0026rsquo;s call them EC2 Cluster. But first, we will create Security Group for EC2 Cluster Create ecs_ec2.tf with the configuration below:\n# ecs_ec2.tf resource \u0026#34;aws_security_group\u0026#34; \u0026#34;ECS_EC2_SG\u0026#34;{ name = \u0026#34;ECS_EC2_SG\u0026#34; description = \u0026#34;Allow traffic from ALB_SG and 22 from Bastion\u0026#34; vpc_id = aws_vpc.main.id ingress = [ { from_port = 49153 to_port = 65535 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [] description = \u0026#34;Allow all traffic coming from port 49153-65535 from ALB\u0026#34; ipv6_cidr_blocks = [] prefix_list_ids = [] security_groups = [aws_security_group.ALB_SG.id] self = false }, { from_port = 32768 to_port = 61000 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [] description = \u0026#34;Allow all traffic coming from port 32768-61000 from ALB\u0026#34; ipv6_cidr_blocks = [] prefix_list_ids = [] security_groups = [aws_security_group.ALB_SG.id] self = false }, { from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [] description = \u0026#34;Allow SSH traffic from Bastion host\u0026#34; ipv6_cidr_blocks = [] prefix_list_ids = [] security_groups = [aws_security_group.Bastion_SG.id] self = false } ] egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = { Name = \u0026#34;ECS EC2 SG\u0026#34; } } With the above Security Group, we have this configuration:\nEC2 Cluster accepts all inbound traffic (from Load Balancer) on range ports 49153-65535 EC2 Cluster accepts all inbound traffic (from Load Balancer) on range ports 32768-61000 EC2 Cluster accepts SSH Protocol from Bastion Host EC2 Cluster allows all outbound traffic You may wonder why we have ranges port 49153-65535 and 32768-61000. The answer is that the container will listen to these range ports after it is successfully launched on the EC2 Cluster. There is a terminology for these ports: ephemeral port.\nFor example, given an ECS Cluster containing 2 EC2 (1) and (2), then:\nOn EC2 (1): there are 3 running containers, which listen on ports 49153, 49154, and 49155, respectively. On EC2 (2): there are 3 running containers, which listen on ports 32769, 32770, and 32771, respectively. Create IAM Role for EC2 Cluster We need to create IAM Role for EC2 Cluster to allow them communicating with the ECS Cluster. Create iamrole.tf with the configuration as below:\ndata \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;ec2_instance_role_policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRole\u0026#34;] effect = \u0026#34;Allow\u0026#34; principals { type = \u0026#34;Service\u0026#34; identifiers = [ \u0026#34;ec2.amazonaws.com\u0026#34;, \u0026#34;ecs.amazonaws.com\u0026#34; ] } } } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;ec2_instance_role_policy\u0026#34; { role = aws_iam_role.ec2_instance_role.name policy_arn = \u0026#34;arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role\u0026#34; } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;ec2_instance_role\u0026#34; { name = \u0026#34;ECS_EC2_InstanceRole\u0026#34; assume_role_policy = data.aws_iam_policy_document.ec2_instance_role_policy.json } resource \u0026#34;aws_iam_instance_profile\u0026#34; \u0026#34;ec2_instance_role_profile\u0026#34; { name = \u0026#34;EC2_InstanceRoleProfile\u0026#34; role = aws_iam_role.ec2_instance_role.id } Create AMI for EC2 Cluster Next, we will create user_data.sh as below. Our EC2 Cluster need to execute the command line below during the process of initialization. We need to configure file /etc/ecs/ecs.config to register these instance into the ECS Cluster.\n#!/bin/bash echo ECS_CLUSTER=\u0026#39;${ecs_cluster_name}\u0026#39; \u0026gt;\u0026gt; /etc/ecs/ecs.config Create ecs_ec2.tf as below:\ndata \u0026#34;aws_ami\u0026#34; \u0026#34;amazon_linux_2\u0026#34; { most_recent = true filter { name = \u0026#34;virtualization-type\u0026#34; values = [\u0026#34;hvm\u0026#34;] } filter { name = \u0026#34;owner-alias\u0026#34; values = [\u0026#34;amazon\u0026#34;] } filter { name = \u0026#34;name\u0026#34; values = [\u0026#34;amzn2-ami-ecs-hvm-*-x86_64-ebs\u0026#34;] } owners = [\u0026#34;amazon\u0026#34;] } ## Launch template for all EC2 instances that are part of the ECS cluster resource \u0026#34;aws_launch_template\u0026#34; \u0026#34;ecs_launch_template\u0026#34; { name = \u0026#34;ECS_EC2_LaunchTemplate\u0026#34; image_id = data.aws_ami.amazon_linux_2.id instance_type = var.ec2_instance_type key_name = aws_key_pair.EC2key.key_name user_data = base64encode(templatefile(\u0026#34;${path.module}/user_data.sh\u0026#34;, {ecs_cluster_name = aws_ecs_cluster.main.name}) ) vpc_security_group_ids = [aws_security_group.ECS_EC2_SG.id] iam_instance_profile { arn = aws_iam_instance_profile.ec2_instance_role_profile.arn } monitoring { enabled = true } } Now, aws_ecs_cluster.main.name is the name of our ECS Cluster, which will be specified in the next section.\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/",
	"title": "Deploy VPC, Jenkins, and SonarQube Server",
	"tags": [],
	"description": "",
	"content": "\rWe use Ops Repo in this section\nTrong phần này, chúng ta sẽ triển khai một VPC với:\n3 Availability Zones 3 Public Subnets 3 Private Subnets 1 Internet Gateway 1 NAT Gateway 3 EC2, where: 1 Jump Host for interacting with the Kubernetes Cluster 1 Jenkins Server for CI pipeline 1 SonarQube Server for DevSecOps Pipeline Content Create VPC Create Jump Host, Jenkins Server, and SonarQube Server "
},
{
	"uri": "//localhost:1313/3-eks/3.2-eks-worker-role/",
	"title": "Validate Scaling Ability",
	"tags": [],
	"description": "",
	"content": "Content: Send 100.000 requests to Load Balancer Send 1.000.000 requests to Load Balancer Scale up Desired Task "
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/2.2.3-jenkins/",
	"title": "Create Auto Scaling Group",
	"tags": [],
	"description": "",
	"content": "Now, we will create Auto Scaling Group for EC2 Cluster. Create asg.tf with the configuration below:\nresource \u0026#34;aws_autoscaling_group\u0026#34; \u0026#34;ecs_autoscaling_group\u0026#34; { name = \u0026#34;ECS_ASG\u0026#34; max_size = 4 min_size = 2 desired_capacity = 2 vpc_zone_identifier = [aws_subnet.private_subnet_1.id, aws_subnet.private_subnet_2.id] health_check_type = \u0026#34;EC2\u0026#34; protect_from_scale_in = true enabled_metrics = [ \u0026#34;GroupMinSize\u0026#34;, \u0026#34;GroupMaxSize\u0026#34;, \u0026#34;GroupDesiredCapacity\u0026#34;, \u0026#34;GroupInServiceInstances\u0026#34;, \u0026#34;GroupPendingInstances\u0026#34;, \u0026#34;GroupStandbyInstances\u0026#34;, \u0026#34;GroupTerminatingInstances\u0026#34;, \u0026#34;GroupTotalInstances\u0026#34; ] launch_template { id = aws_launch_template.ecs_launch_template.id version = \u0026#34;$Latest\u0026#34; } instance_refresh { strategy = \u0026#34;Rolling\u0026#34; } tag { key = \u0026#34;Name\u0026#34; value = \u0026#34;ASG For ECS\u0026#34; propagate_at_launch = true } } With the configuration above, Auto Scaling Group (ASG) will launch EC2 Cluster into private subnet 1 and 2 based on the AMI, which is defined in the previous section. ASG guarantees that at least there are 2 EC2 running at the same time. In case of scaling up, there will be 4 EC2 Instances in the ECS Cluster. However, we need to configure that. In asg.tf, adding the configuration as below:\nresource \u0026#34;aws_autoscaling_policy\u0026#34; \u0026#34;asg_policy_ecs_autoscaling_group\u0026#34; { autoscaling_group_name = aws_autoscaling_group.ecs_autoscaling_group.name name = \u0026#34;Custom Policy\u0026#34; policy_type = \u0026#34;TargetTrackingScaling\u0026#34; target_tracking_configuration { predefined_metric_specification { predefined_metric_type = \u0026#34;ASGAverageCPUUtilization\u0026#34; } target_value = 50.0 } } In 3 minutes, if CPU Utilization reachs 50%, ASG will launch another EC2 Instance to reduce the heavy workload.\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.1-vpc/2.1.3-run-terraform/",
	"title": "Run Association",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/3-eks/",
	"title": "Validate Scaling",
	"tags": [],
	"description": "",
	"content": "In this step, we will validate the scalability of the infrastructure.\nContent Connect to Bastion Host Validate Scaling Ability "
},
{
	"uri": "//localhost:1313/4-cicd/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Using Terminal and executing this command lines:\nterraform destroy Now, all resources will be removed as below:\n"
},
{
	"uri": "//localhost:1313/5-finish-monitoring/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Using Terminal and executing this command lines:\nterraform destroy Now, all resources will be removed as below:\n"
},
{
	"uri": "//localhost:1313/6-devsecops/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Using Terminal and executing this command lines:\nterraform destroy Now, all resources will be removed as below:\n"
},
{
	"uri": "//localhost:1313/7-argocd-autodeploy/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Using Terminal and executing this command lines:\nterraform destroy Now, all resources will be removed as below:\n"
},
{
	"uri": "//localhost:1313/8-cicd-test/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Using Terminal and executing this command lines:\nterraform destroy Now, all resources will be removed as below:\n"
},
{
	"uri": "//localhost:1313/4-cicd/4.1-application/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/5-finish-monitoring/5.1-prome-grafana/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/6-devsecops/6.1-ops-repo/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/7-argocd-autodeploy/7.1-ci-pipeline/",
	"title": "Connect to Bastion Host",
	"tags": [],
	"description": "",
	"content": "\nSSH Agent Forwading We can connect to EC2 Cluster in private subnet through Bastion Host. However, the last thing we want to do is placing our private key on the Bastion Host. So, we need to use SSH Agent Forwarding. At the folder containing the private key, executing the command line below:\nssh-add EC2.pem Then, we connect to the Bastion Host by:\nssh -A ubuntu@\u0026lt;your-bastion-host-public-IP\u0026gt; We can connect to our EC2 Cluster by using this command line:\nssh ec2-user@\u0026lt;your-EC2Cluster-private-IP\u0026gt; Validate Scaling Ability Although this is not the main function of the bastion host. However, you can use Bastion Host to test the scaling ability because of its convenience. Let\u0026rsquo;s validate the scaling ability by sending request to the Load Balancer.\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/2.2.4-sonar/",
	"title": "Create ECS Cluster",
	"tags": [],
	"description": "",
	"content": "Create ECS Cluster Create ecs.tf with the configuration below:\nresource \u0026#34;aws_ecs_cluster\u0026#34; \u0026#34;main\u0026#34; { name = var.ecs_cluster_name tags = { Name = var.ecs_cluster_name } } The name of the ECS Cluster is the variable var.ecs_cluster_name. Follow this link for checking all defined variables.\nCreate Task Definition Next, we will create Task Definition. In ecs.tf, adding the configuration as below:\nresource \u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;my_ECS_TD\u0026#34; { family = \u0026#34;ECS_TaskDefinition\u0026#34; container_definitions = jsonencode([ { name = var.container_name image = var.image_name cpu = 500 memory = 500 essential = true portMappings = [ { containerPort = var.container_port hostPort = 0 protocol = \u0026#34;tcp\u0026#34; } ] } ]) } Our Task Definition contains a container, which is launched from a Docker Image from Docker Hub. This container exposes port var.container_port (3000). When a container running on an EC2 Cluster, it will bind the port of the container to the one of ports in range 49153-65535 or 32768-61000 on the EC2 Cluster. And the most important is we can allocate resources for the container:\n0.5 vCPU for the task (This task contains only one container) 500 MiB of memory for this task (This task contains only one container container) So, we solved the problem of allocating computing resources.\nCreate ECS Service Next, we will crete ECS Service (Service will be responsible for launching task). But first, we need to create IAM Role for Service to allow them interacting with EC2 Cluster and Load Balancer. In iamrole.tf, adding the configuration as below:\n# iamrole.tf data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;ecs_service_policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRole\u0026#34;] effect = \u0026#34;Allow\u0026#34; principals { type = \u0026#34;Service\u0026#34; identifiers = [\u0026#34;ecs.amazonaws.com\u0026#34;,] } } } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;ecs_service_role\u0026#34; { name = \u0026#34;ECS_ServiceRole\u0026#34; assume_role_policy = data.aws_iam_policy_document.ecs_service_policy.json } resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;ecs_service_role_policy\u0026#34; { name = \u0026#34;ECS_ServiceRolePolicy\u0026#34; policy = data.aws_iam_policy_document.ecs_service_role_policy.json role = aws_iam_role.ecs_service_role.id } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;ecs_service_role_policy\u0026#34; { statement { effect = \u0026#34;Allow\u0026#34; actions = [ \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;elasticloadbalancing:DeregisterInstancesFromLoadBalancer\u0026#34;, \u0026#34;elasticloadbalancing:DeregisterTargets\u0026#34;, \u0026#34;elasticloadbalancing:Describe*\u0026#34;, \u0026#34;elasticloadbalancing:RegisterInstancesWithLoadBalancer\u0026#34;, \u0026#34;elasticloadbalancing:RegisterTargets\u0026#34;, \u0026#34;ec2:DescribeTags\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:PutSubscriptionFilter\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ] resources = [\u0026#34;*\u0026#34;] } } Finally, we will create ECS Service. In ecs.tf, adding the configuration as below:\nresource \u0026#34;aws_ecs_service\u0026#34; \u0026#34;service\u0026#34; { name = \u0026#34;ECS_Service\u0026#34; iam_role = aws_iam_role.ecs_service_role.arn cluster = aws_ecs_cluster.main.id task_definition = aws_ecs_task_definition.my_ECS_TD.arn desired_count = var.ecs_task_desired_count load_balancer { target_group_arn = aws_alb_target_group.alb_target_group.arn container_name = var.container_name container_port = var.container_port } ## Spread tasks evenly accross all Availability Zones for High Availability ordered_placement_strategy { type = \u0026#34;spread\u0026#34; field = \u0026#34;attribute:ecs.availability-zone\u0026#34; } ## Make use of all available space on the Container Instances ordered_placement_strategy { type = \u0026#34;binpack\u0026#34; field = \u0026#34;memory\u0026#34; } # Do not update desired count again to avoid a reset to this number on every deploymengit t lifecycle { ignore_changes = [desired_count] } } With the above configuration, we have:\nECS Service belonging to the defined ECS Cluster IAM Role for ECS Service Task Definition in ECS Service Task (Container(s)) will be launched into EC2 Cluster and registered into the Target Group. So, Load Balancer can forward traffic to them. Task will be distributed across Availability Zones to ensure the High Availability In case of increasing the number of desired task, ECS Service guarantees using all available space on the EC2 Cluster. Now, we have a completed AWS Infrastructure.\n"
},
{
	"uri": "//localhost:1313/2-vpc-ec2/2.2-jump-jenkins-sonar/2.2.5-run-terraform/",
	"title": "Create ECS Cluster",
	"tags": [],
	"description": "",
	"content": "Create ECS Cluster Create ecs.tf with the configuration below:\nresource \u0026#34;aws_ecs_cluster\u0026#34; \u0026#34;main\u0026#34; { name = var.ecs_cluster_name tags = { Name = var.ecs_cluster_name } } The name of the ECS Cluster is the variable var.ecs_cluster_name. Follow this link for checking all defined variables.\nCreate Task Definition Next, we will create Task Definition. In ecs.tf, adding the configuration as below:\nresource \u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;my_ECS_TD\u0026#34; { family = \u0026#34;ECS_TaskDefinition\u0026#34; container_definitions = jsonencode([ { name = var.container_name image = var.image_name cpu = 500 memory = 500 essential = true portMappings = [ { containerPort = var.container_port hostPort = 0 protocol = \u0026#34;tcp\u0026#34; } ] } ]) } Our Task Definition contains a container, which is launched from a Docker Image from Docker Hub. This container exposes port var.container_port (3000). When a container running on an EC2 Cluster, it will bind the port of the container to the one of ports in range 49153-65535 or 32768-61000 on the EC2 Cluster. And the most important is we can allocate resources for the container:\n0.5 vCPU for the task (This task contains only one container) 500 MiB of memory for this task (This task contains only one container container) So, we solved the problem of allocating computing resources.\nCreate ECS Service Next, we will crete ECS Service (Service will be responsible for launching task). But first, we need to create IAM Role for Service to allow them interacting with EC2 Cluster and Load Balancer. In iamrole.tf, adding the configuration as below:\n# iamrole.tf data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;ecs_service_policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRole\u0026#34;] effect = \u0026#34;Allow\u0026#34; principals { type = \u0026#34;Service\u0026#34; identifiers = [\u0026#34;ecs.amazonaws.com\u0026#34;,] } } } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;ecs_service_role\u0026#34; { name = \u0026#34;ECS_ServiceRole\u0026#34; assume_role_policy = data.aws_iam_policy_document.ecs_service_policy.json } resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;ecs_service_role_policy\u0026#34; { name = \u0026#34;ECS_ServiceRolePolicy\u0026#34; policy = data.aws_iam_policy_document.ecs_service_role_policy.json role = aws_iam_role.ecs_service_role.id } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;ecs_service_role_policy\u0026#34; { statement { effect = \u0026#34;Allow\u0026#34; actions = [ \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;elasticloadbalancing:DeregisterInstancesFromLoadBalancer\u0026#34;, \u0026#34;elasticloadbalancing:DeregisterTargets\u0026#34;, \u0026#34;elasticloadbalancing:Describe*\u0026#34;, \u0026#34;elasticloadbalancing:RegisterInstancesWithLoadBalancer\u0026#34;, \u0026#34;elasticloadbalancing:RegisterTargets\u0026#34;, \u0026#34;ec2:DescribeTags\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:PutSubscriptionFilter\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ] resources = [\u0026#34;*\u0026#34;] } } Finally, we will create ECS Service. In ecs.tf, adding the configuration as below:\nresource \u0026#34;aws_ecs_service\u0026#34; \u0026#34;service\u0026#34; { name = \u0026#34;ECS_Service\u0026#34; iam_role = aws_iam_role.ecs_service_role.arn cluster = aws_ecs_cluster.main.id task_definition = aws_ecs_task_definition.my_ECS_TD.arn desired_count = var.ecs_task_desired_count load_balancer { target_group_arn = aws_alb_target_group.alb_target_group.arn container_name = var.container_name container_port = var.container_port } ## Spread tasks evenly accross all Availability Zones for High Availability ordered_placement_strategy { type = \u0026#34;spread\u0026#34; field = \u0026#34;attribute:ecs.availability-zone\u0026#34; } ## Make use of all available space on the Container Instances ordered_placement_strategy { type = \u0026#34;binpack\u0026#34; field = \u0026#34;memory\u0026#34; } # Do not update desired count again to avoid a reset to this number on every deploymengit t lifecycle { ignore_changes = [desired_count] } } With the above configuration, we have:\nECS Service belonging to the defined ECS Cluster IAM Role for ECS Service Task Definition in ECS Service Task (Container(s)) will be launched into EC2 Cluster and registered into the Target Group. So, Load Balancer can forward traffic to them. Task will be distributed across Availability Zones to ensure the High Availability In case of increasing the number of desired task, ECS Service guarantees using all available space on the EC2 Cluster. Now, we have a completed AWS Infrastructure.\n"
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]